{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unusual-portugal",
   "metadata": {},
   "source": [
    "# [TF.Text](https://www.tensorflow.org/tutorials/tensorflow_text/intro)\n",
    "\n",
    "TensorFlow Text はテキストデータに関するクラスとメソッドを提供している。ライブラリはテキストの事前処理などのTensorFlowの大元が提供していないこともできる。TensorFlow graph を使えることも利点である。TensorFlow Text を使っていれば、トークン化や事前処理などを心配する必要がない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worse-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "charitable-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most ops expect that the strings are in UTF-8.\n",
    "# If you're using a different encoding, you can use the core tensorflow transcode op to transcode into UTF-8.\n",
    "docs = tf.constant([u'Everything not saved will be lost.'.encode('UTF-16-BE'), u'Sad☹'.encode('UTF-16-BE')])\n",
    "\n",
    "# You can also use the same op to coerce your string to structurally valid UTF-8 if your input could be invalid.\n",
    "utf8_docs = tf.strings.unicode_transcode(docs, input_encoding='UTF-16-BE', output_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-digest",
   "metadata": {},
   "source": [
    "## トークン化\n",
    "\n",
    "文字列をトークン（単語、数字、句読点）に分割する。`Tokenizer`や`TokenizerWithOffsets`インターフェースを使う。これらはそれぞれメソッドとして、`tokenize`と`tokenize_with_offsets`を持つ。いくつかのトークン化手法を利用することができ、それぞれ`TokenizerWithOffsets`（`Tokenizer`の拡張）を実装している。これらはそれぞれバイトオフセットを元の文字列に取り入れるオプションを含み、これを使うことによって、トークンが作成された元の文字列にあるバイトを知ることができる。\n",
    "\n",
    "すべてのトークナイザーは、元の個々の文字列にマッピングされたトークンの最も内側の次元を持つ`RaggedTensor`を返します。 その結果、結果のシェイプのランクが1つ上がります。 それらに慣れていない場合は、[ragged_tensors](https://www.tensorflow.org/guide/ragged_tensors)を確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "upset-stranger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/nakayamayasuaki/python_env/dl_env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n",
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost.'], [b'Sad\\xe2\\x98\\xb9']]\n"
     ]
    }
   ],
   "source": [
    "# Split UTF-8 strings on ICU defined whitespace characters (e.g. space, tab, newline).\n",
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "emerging-helping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xe2\\x98\\xb9']]\n"
     ]
    }
   ],
   "source": [
    "# Split UTF-8 strings based on Unicode script boundaries.\n",
    "tokenizer = text.UnicodeScriptTokenizer()\n",
    "tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "announced-banking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'\\xe4\\xbb\\x85', b'\\xe4\\xbb\\x8a', b'\\xe5\\xb9\\xb4', b'\\xe5\\x89\\x8d']]\n"
     ]
    }
   ],
   "source": [
    "# Split by character (common to use when tokenizing languages without whitespace).\n",
    "tokens = tf.strings.unicode_split([u\"仅今年前\".encode('UTF-8')], 'UTF-8')\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "brazilian-change",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xe2\\x98\\xb9']]\n",
      "[[0, 11, 15, 21, 26, 29, 33], [0, 3]]\n",
      "[[10, 14, 20, 25, 28, 33, 34], [3, 6]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.UnicodeScriptTokenizer()\n",
    "\n",
    "# Since it is desired to know where in the original string the token originated from,\n",
    "# each tokenizer which implements `TokenizerWithOffsets` has a `tokenize_with_offsets`method\n",
    "# that will return the byte offsets along with the tokens.\n",
    "(tokens, start_offsets, end_offsets) = tokenizer.tokenize_with_offsets([\n",
    "    'everything not saved will be lost.',\n",
    "    u'Sad☹'.encode('UTF-8')\n",
    "])\n",
    "\n",
    "print(tokens.to_list())\n",
    "# `start_offsets` lists the bytes in the original string each token starts at (inclusive).\n",
    "print(start_offsets.to_list())\n",
    "# `end_offsets` lists the bytes immediately after the point where each token ends (exclusive).\n",
    "print(end_offsets.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "proved-hypothetical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'Never', b'tell', b'me', b'the', b'odds.']]\n",
      "[[b\"It's\", b'a', b'trap!']]\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "docs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'], [\"It's a trap!\"]])\n",
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n",
    "iterator = iter(tokenized_docs)\n",
    "print(next(iterator).to_list())\n",
    "print(next(iterator).to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-pixel",
   "metadata": {},
   "source": [
    "## そのほか\n",
    "\n",
    "TF.Text パッケージはほかにも便利な事前処理方法を提供している。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-bermuda",
   "metadata": {},
   "source": [
    "文章の分割モデルは文字が大文字なのか小文字なのか、句読点が文末なのか文中なのかなどの特徴を見る場合がある。`Wordshape`は入力に対して多くのパターンをマッチングさせるための機能を持つ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "measured-louisville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[True, False, False, False, False, False], [True]]\n",
      "[[False, False, False, False, False, False], [False]]\n",
      "[[False, False, False, False, False, True], [True]]\n",
      "[[False, False, False, False, False, False], [False]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "\n",
    "# Is capitalized?\n",
    "f1 = text.wordshape(tokens, text.WordShape.HAS_TITLE_CASE)\n",
    "# Are all letters uppercased?\n",
    "f2 = text.wordshape(tokens, text.WordShape.IS_UPPERCASE)\n",
    "# Does the token contain punctuation?\n",
    "f3 = text.wordshape(tokens, text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL)\n",
    "# Is the token a number?\n",
    "f4 = text.wordshape(tokens, text.WordShape.IS_NUMERIC_VALUE)\n",
    "\n",
    "print(f1.to_list())\n",
    "print(f2.to_list())\n",
    "print(f3.to_list())\n",
    "print(f4.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-stock",
   "metadata": {},
   "source": [
    "N-gramsも利用することができる。これはサイズ$n$のスライディングウィンドウが与えられた時に得られる単語の配列である。トークンを組み合わせる時、三つの削減メカニズムがサポートされている。テキストについては、`Reduction.STRING_JOIN`を使って文字列を結合することができる。デフォルトのセパレータはスペースである。他の削減メカニズム`Reduction.SUM`と`Reduction.MEAN`は主に数値の場合に利用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reflected-photography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'Everything not', b'not saved', b'saved will', b'will be', b'be lost.'], []]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "\n",
    "# Ngrams, in this case bi-gram (n = 2)\n",
    "bigrams = text.ngrams(tokens, 2, reduction_type=text.Reduction.STRING_JOIN)\n",
    "\n",
    "print(bigrams.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-hampshire",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
