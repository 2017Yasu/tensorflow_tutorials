{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alternative-potter",
   "metadata": {},
   "source": [
    "# [部分単語のトークン化](https://www.tensorflow.org/tutorials/tensorflow_text/subwords_tokenizer)\n",
    "\n",
    "データセットからサブワード語彙を生成し、それらを使って`text.BertTokenizer`を語彙から構築する。\n",
    "\n",
    "サブワードトークナイザの主な利点は、単語ベースのトークン化と文字ベースのトークン化の間に加筆することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "modern-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "informed-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "pwd = pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-police",
   "metadata": {},
   "source": [
    "`tensorflow_text`は三つのサブワードスタイルのトークナイザを持っている。\n",
    "\n",
    "- `text.BertTokenizer`: ハイレベルインターフェースのクラス。BERTのトークン分割アルゴリズムと`WordpiceTokenizer`を含む。文章を入力として受け取り、トークンIDを返す。\n",
    "- `text.WordpieceTokenizer`: レベルの低いインターフェースのクラス。[WordPiece algorithm](https://www.tensorflow.org/tutorials/tensorflow_text/subwords_tokenizer#applying_wordpiece)のみ実装されている。呼び出す前にテキストを標準化して単語に分割する必要がある。入力として単語を受け取り、トークンIDを返す。\n",
    "- `text.SentencepieceTokenizer`: より複雑な設定が必要。初期化メソッドは学習前の*sentencepiece*モデルを必要とする。構築のやり方は[sentencepiece repository](https://github.com/google/sentencepiece#train-sentencepiece-model)を参照。トークン化する際に、入力として文章を受け付ける。\n",
    "\n",
    "ここでは、トップダウン方式で既存の単語群から*wordpiece vocabulary*を構築する。これは日本語、中国語、韓国語には適用できない（これらの言語のトークン化には`text.SentencepieceTokenizer`、`text.UnicodeCharTokenizer`、もしくは[このアプローチ](https://tfhub.dev/google/zh_segmentation/1)を使う）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-hands",
   "metadata": {},
   "source": [
    "## データセットのダウンロード\n",
    "\n",
    "ここではポルトガル語と英語の翻訳データセットをtfdsからダウンロードする。このデータセットは全て小文字で、句読点の周りにはスペースがあり、なんのユニコード標準化手法が使われているかは不明である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incorporate-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "armed-patrick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portuguese:  e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "English:    and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n"
     ]
    }
   ],
   "source": [
    "for pt, en in train_examples.take(1):\n",
    "    print(\"Portuguese: \", pt.numpy().decode('utf-8'))\n",
    "    print(\"English:   \", en.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "liberal-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = train_examples.map(lambda pt, en: en)\n",
    "train_pt = train_examples.map(lambda pt, en: pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-perspective",
   "metadata": {},
   "source": [
    "## ボキャブラリーを生成する\n",
    "\n",
    "データセットから*wordpiece*ボキャブラリーを生成する。すでにボキャブラリーのファイルがある場合にはこのステップは必要ない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unsigned-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "amateur-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 8000,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "proprietary-transcript",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function regex_split_with_offsets at 0x1672cf790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "CPU times: user 1min 38s, sys: 2.54 s, total: 1min 40s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pt_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_pt.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "continent-cornwall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n",
      "['no', 'por', 'mais', 'na', 'eu', 'esta', 'muito', 'isso', 'isto', 'sao']\n",
      "['90', 'desse', 'efeito', 'malaria', 'normalmente', 'palestra', 'recentemente', '##nca', 'bons', 'chave']\n",
      "['##–', '##—', '##‘', '##’', '##“', '##”', '##⁄', '##€', '##♪', '##♫']\n"
     ]
    }
   ],
   "source": [
    "print(pt_vocab[:10])\n",
    "print(pt_vocab[100:110])\n",
    "print(pt_vocab[1000:1010])\n",
    "print(pt_vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "plain-study",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 826 ms, total: 1min 14s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_en.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "usual-aging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n",
      "['as', 'all', 'at', 'one', 'people', 're', 'like', 'if', 'our', 'from']\n",
      "['choose', 'consider', 'extraordinary', 'focus', 'generation', 'killed', 'patterns', 'putting', 'scientific', 'wait']\n",
      "['##_', '##`', '##ย', '##ร', '##อ', '##–', '##—', '##’', '##♪', '##♫']\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab[:10])\n",
    "print(en_vocab[100:110])\n",
    "print(en_vocab[1000:1010])\n",
    "print(en_vocab[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-poultry",
   "metadata": {},
   "source": [
    "ボキャブラリーファイルを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "female-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath, 'w') as f:\n",
    "        for token in vocab:\n",
    "            print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "suited-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file('pt_vacab.txt', pt_vocab)\n",
    "write_vocab_file('en_vacab.txt', en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fabulous-reggae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_vacab.txt  pt_vacab.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls *.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-cornell",
   "metadata": {},
   "source": [
    "## トークナイザの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-illustration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
