{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coral-concern",
   "metadata": {},
   "source": [
    "# [映画レビューのテキスト分類](https://www.tensorflow.org/tutorials/keras/text_classification?hl=ja)\n",
    "\n",
    "映画のレビューをそのテキストを使って肯定的か否定的かに分類する（二値分類あるいは2クラス分類という問題の例）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-finnish",
   "metadata": {},
   "source": [
    "Internet Movie Databaseから抽出した50,000件の映画レビューを含むIMDB datasetを利用。訓練用とテスト用にそれぞれ25,000件用意されている。肯定的なレビューと否定的なレビューが半々で存在する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "powered-capitol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "voluntary-tribe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf14/lib/python3.7/site-packages/tensorflow_core/python/keras/datasets/imdb.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/opt/anaconda3/envs/tf14/lib/python3.7/site-packages/tensorflow_core/python/keras/datasets/imdb.py:130: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "imdb = keras.datasets.imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-section",
   "metadata": {},
   "source": [
    "tensorflowにパッケージ化されているIMDBデータセットは前処理済み。レビューが辞書中の特定の単語に対応する整数の配列に変換されている。`new_words=1000`は訓練データ中に出てくる単語のうち、最も頻繁に出現する10,000個を保持するためのもの。データサイズを管理可能にするため、稀にしか出現しない単語は破棄される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "public-opening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 25000, labels: 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "occupational-blake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "based-seven",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 189)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-prophet",
   "metadata": {},
   "source": [
    "整数を文字列にマッピングする辞書オブジェクトを検索するための関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "thick-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict() that maps words to integers\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# The first 3 indices are already fixed\n",
    "word_index = {k: (v + 3) for k, v in word_index.items()} \n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "trained-imagination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-conversation",
   "metadata": {},
   "source": [
    "データの準備\n",
    "\n",
    "データはNNに投入する前に、テンソルに変換する。\n",
    "- method 1: one-hotエンコーディングと同じように、単語の出現を表す0と1のベクトルに変換（例：\\[3, 5\\]という配列は、インデックス3と5を除いてすべて0の10,000次元のベクトルになる）。この時、最初の層は浮動小数店のベクトルデータを扱うことができるDense層とする。\n",
    "- method 2: 配列をパディングによって同じ長さに揃え、（サンプル数）*（長さの最大値）の形の整数テンソルにする。この時、最初の層はこの形式を扱うことができるEmbedding層とする。\n",
    "\n",
    "ここではmethod 2を採用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "labeled-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 長さを標準化する\n",
    "\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(\n",
    "    train_data,\n",
    "    value=word_index[\"<PAD>\"],\n",
    "    padding='post',\n",
    "    maxlen=256\n",
    ")\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(\n",
    "    test_data,\n",
    "    value=word_index[\"<PAD>\"],\n",
    "    padding='post',\n",
    "    maxlen=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "known-petersburg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "express-track",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      " 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
      "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
      "   52    5   14  407   16   82    2    8    4  107  117 5952   15  256\n",
      "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
      "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
      "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
      "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
      "  103   32   15   16 5345   19  178   32    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-beauty",
   "metadata": {},
   "source": [
    "モデルの構築\n",
    "\n",
    "- 入力データ：単語インデックスの配列\n",
    "- 出力：ラベル（0 or 1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "worldwide-philadelphia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-charleston",
   "metadata": {},
   "source": [
    "- Layer 1: `Embedding(input_dim, output_dim)`層。整数にエンコードされた語彙を受け取り、それぞれの単語インデックスに対応する埋め込みベクトルを検索する。埋め込みベクトルはモデルの訓練の中で学習される。ベクトル化のため、出力行列には次元が1追加される（Input shape: `(batch_size, input_length)`, Output shape: `(batch_size, input_length, output_dim)`）。\n",
    "- Layer 2: `GlobalAveragePooling1D()`層。それぞれのサンプルについて、シーケンスの次元方向に平均値を求め、固定長のベクトルを返す。この結果、モデルは最も単純な形で可変長の入力を扱うことができる。Input shape: `(batch_size, steps, features)`, Output shape: `(batch_size, features)`\n",
    "- Layer 3: `Dense(units)`層。16個のノードを持つ全結合層の隠れユニット。隠れユニットのノード数は、モデルが学習によって獲得する内部表現の自由度を決定する。多くの隠れユニットがある場合、または多くの層がある場合、モデルはより複雑な内部表現を学習することができる。一方で、計算量が多くなり、学習してほしくないパターンを学習してしまう可能性がある（過学習）。\n",
    "- Layer 4: `Dense(units)`層。1個のノードを持つ全結合層。Sigmoid関数を利用することで、出力値は0から1の確率（あるいは確信度）で表される。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-brass",
   "metadata": {},
   "source": [
    "損失関数とオプティマイザ\n",
    "\n",
    "今回の問題は二値分類問題であり、モデルの出力は確率であるため、損失関数には`binary_crossentropy`（二値のクロスエントロピー）関数を使用。`binary_crossentropy`は、確率分布の間の「距離」を測定する関数であるので、今回のような確率を扱う場合に適している。他には、`mean_squared_error`（平均二乗誤差）を使うこともできる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "involved-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-apache",
   "metadata": {},
   "source": [
    "訓練を行う際に、モデルが見ていないデータ（検証データ：validation set）での正解率を検証する。元の訓練データから10000個のサンプルを取り出し、それらを利用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "straight-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-principal",
   "metadata": {},
   "source": [
    "モデルの訓練\n",
    "\n",
    "512個のサンプルからなるミニバッチを使って、40エポックモデルを訓練する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-profession",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    partial_x_train,\n",
    "    partial_y_train,\n",
    "    epochs=40,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the trained model\n",
    "\n",
    "results = model.evaluate(test_data, test_labels, verbose=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-military",
   "metadata": {},
   "source": [
    "正解率と損失の時系列グラフ\n",
    "\n",
    "`model.fit()`は、訓練中に発生した全てのことを記録した辞書を含む`History`オブジェクトを返す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 図のクリア\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-montreal",
   "metadata": {},
   "source": [
    "上記のグラフでは、点が訓練時の損失と正解率を、実線が検証時の損失と正解率を表しています。\n",
    "\n",
    "訓練時の損失がエポックごとに**減少**し、訓練時の正解率がエポックごとに**上昇**していることに気がつくはずです。繰り返すごとに指定された数値指標を最小化する勾配降下法を最適化に使用している場合に期待される動きです。\n",
    "\n",
    "これは、検証時の損失と正解率には当てはまりません。20エポックを過ぎたあたりから、横ばいになっているようです。これが、過学習の一例です。モデルの性能が、訓練用データでは高い一方で、見たことの無いデータではそれほど高くないというものです。このポイントをすぎると、モデルが最適化しすぎて、訓練用データでは特徴的であるが、テスト用データには一般化できない内部表現を学習しています。\n",
    "\n",
    "このケースの場合、20エポックを過ぎたあたりで訓練をやめることで、過学習を防止することが出来ます。後ほど、コールバックを使って、これを自動化する方法を紹介します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-idaho",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tf14)",
   "language": "python",
   "name": "conda_tf14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
