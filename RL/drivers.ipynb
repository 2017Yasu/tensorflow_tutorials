{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "external-method",
   "metadata": {},
   "source": [
    "# [ドライバ](https://www.tensorflow.org/agents/tutorials/4_drivers_tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tired-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers import dynamic_episode_driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-beach",
   "metadata": {},
   "source": [
    "## Pythonドライバ\n",
    "\n",
    "`PyDriver`クラスは、Python環境、Pythonポリシー、および各ステップで更新するオブザーバーのリストを受け取ります。主なメソッドは`run()`で、終了基準（ステップ数が`max_steps` に達するか、エピソード数が`max_episodes`に達する）が少なくとも1つ満たされるまで、ポリシーに基づいた行動を環境で実行します。\n",
    "\n",
    "CartPole環境での実行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "silent-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "policy = random_py_policy.RandomPyPolicy(time_step_spec=env.time_step_spec(), \n",
    "                                         action_spec=env.action_spec())\n",
    "replay_buffer = []\n",
    "metric = py_metrics.AverageReturnMetric()\n",
    "observers = [replay_buffer.append, metric]\n",
    "driver = py_driver.PyDriver(\n",
    "    env, policy, observers, max_steps=20, max_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "passive-extension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer:\n",
      "Trajectory(step_type=array(0, dtype=int32), observation=array([ 0.03291266, -0.02432417,  0.00065954,  0.02049842], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.03242618,  0.17078832,  0.00106951, -0.27197635], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.03584194, -0.02434888, -0.00437002,  0.02104372], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.03535496,  0.17083547, -0.00394915, -0.27301478], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.03877167, -0.02422991, -0.00940944,  0.01841996], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.03828708, -0.21921566, -0.00904104,  0.3081193 ], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.03390276, -0.41420764, -0.00287866,  0.5979373 ], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.02561861, -0.21904552,  0.00908009,  0.304349  ], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.0212377 , -0.02405414,  0.01516707,  0.01454349], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.02075662,  0.17084706,  0.01545794, -0.27331573], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.02417356, -0.02449201,  0.00999163,  0.02420232], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.02368372,  0.17048524,  0.01047567, -0.26531145], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.02709342,  0.3654561 ,  0.00516944, -0.55467194], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.03440255,  0.17026196, -0.005924  , -0.26036483], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.03780778,  0.36546797, -0.01113129, -0.55491036], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.04511714,  0.56074446, -0.0222295 , -0.8510794 ], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.05633203,  0.36593252, -0.03925109, -0.56546867], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.06365068,  0.17138259, -0.05056046, -0.2854054 ], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.06707834, -0.02298317, -0.05626857, -0.00908778], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.06661867,  0.17289871, -0.05645033, -0.31897983], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Average Return:  0.0\n"
     ]
    }
   ],
   "source": [
    "initial_time_step = env.reset()\n",
    "final_time_step, _ = driver.run(initial_time_step)\n",
    "\n",
    "print('Replay Buffer:')\n",
    "for traj in replay_buffer:\n",
    "    print(traj)\n",
    "\n",
    "print('Average Return: ', metric.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-wings",
   "metadata": {},
   "source": [
    "## TensorFlowドライバ\n",
    "\n",
    "また、TensorFlowには、Pythonドライバと機能的に似ているドライバがありますが、TensorFlowドライバでは、TF環境、TFポリシー、TFオブザーバーなどを使用します。現在、次の2つのTensorFlowドライバがあります。\n",
    "\n",
    "- `DynamicStepDriver`：指定された数の（有効な）環境ステップの後に終了します。\n",
    "- `DynamicEpisodeDriver`：指定された数のエピソードの後に終了します。\n",
    "\n",
    "実際の`DynamicEpisodeDriver`の例を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "corporate-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "tf_policy = random_tf_policy.RandomTFPolicy(action_spec=tf_env.action_spec(),\n",
    "                                            time_step_spec=tf_env.time_step_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "floral-income",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_time_step TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.00442522, -0.04254914, -0.01799237,  0.03740452]],\n",
      "      dtype=float32)>)\n",
      "Number of Steps:  39\n",
      "Number of Episodes:  2\n"
     ]
    }
   ],
   "source": [
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "observers = [num_episodes, env_steps]\n",
    "driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env, tf_policy, observers, num_episodes=2)\n",
    "\n",
    "# Initial driver.run will reset the environment and initialize the policy.\n",
    "final_time_step, policy_state = driver.run()\n",
    "\n",
    "print('final_time_step', final_time_step)\n",
    "print('Number of Steps: ', env_steps.result().numpy())\n",
    "print('Number of Episodes: ', num_episodes.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "subjective-accident",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_time_step TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.03343501, -0.0185016 ,  0.0439446 , -0.04853262]],\n",
      "      dtype=float32)>)\n",
      "Number of Steps:  67\n",
      "Number of Episodes:  4\n"
     ]
    }
   ],
   "source": [
    "# Continue running from previous state\n",
    "final_time_step, _ = driver.run(final_time_step, policy_state)\n",
    "\n",
    "print('final_time_step', final_time_step)\n",
    "print('Number of Steps: ', env_steps.result().numpy())\n",
    "print('Number of Episodes: ', num_episodes.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-painting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
