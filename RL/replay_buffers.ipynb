{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "centered-digit",
   "metadata": {},
   "source": [
    "# [リプレイバッファ](https://www.tensorflow.org/agents/tutorials/5_replay_buffers_tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "norman-presence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents import specs\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.replay_buffers import py_uniform_replay_buffer\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-hearing",
   "metadata": {},
   "source": [
    "## リプレイバッファAPI\n",
    "\n",
    "`ReplayBuffer`クラスのオブジェクトが初期化される際には、そのオブジェクトが格納する要素の `data_spec` が必要です。この仕様は、バッファに追加されるトラジェクトリ要素の `TensorSpec` に対応しています。この仕様は通常、トレーニング時にエージェントが期待する形状、タイプ、構造を定義するエージェントの `agent.collect_data_spec` を調査することで取得できます（詳細は後述します）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-obligation",
   "metadata": {},
   "source": [
    "## `TFUniformReplayBuffer`\n",
    "\n",
    "これは `TF-Agents` で最も一般的に使用される再生バッファであるため、このチュートリアルではこの再生バッファを使用します。`TFUniformReplayBuffer` では、バッキングバッファの記憶は `tensorflow` 変数によって行われるため、計算グラフの一部になっています。\n",
    "\n",
    "バッファには複数の要素がバッチ単位で格納され、バッチセグメントごとに最大容量の `max_length` 要素があります。したがって、合計バッファ容量は、`batch_size * max_length` 要素となります。バッファに格納される要素はすべて、対応するデータ仕様を持っている必要があります。再生バッファがデータ収集に使用される場合、仕様はエージェントの収集データ仕様になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-american",
   "metadata": {},
   "source": [
    "### バッファの作成\n",
    "\n",
    "`TFUniformReplayBuffer` を作成するには、次の値を渡します。\n",
    "\n",
    "1. バッファが格納するデータ要素の仕様\n",
    "2. バッファのバッチサイズ`batch_size`\n",
    "3. バッチセグメントごとの要素数`max_length`\n",
    "\n",
    "サンプルデータの仕様（`batch_size=32` および `max_length=1000`）を使用して `TFUniformReplayBuffer` を作成する例を以下に示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fresh-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spec =  (\n",
    "    tf.TensorSpec([3], tf.float32, 'action'),\n",
    "    (\n",
    "        tf.TensorSpec([5], tf.float32, 'lidar'),\n",
    "        tf.TensorSpec([3, 2], tf.float32, 'camera')\n",
    "    )\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "max_length = 1000\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec,\n",
    "    batch_size=batch_size,\n",
    "    max_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-graph",
   "metadata": {},
   "source": [
    "### バッファへの書き込み\n",
    "\n",
    "再生バッファに要素を追加するため `add_batch(items)` メソッドを使用します。`items`はバッチを表すテンソルのリスト/タプル/ネストになっている。 `items` の各要素には外側の次元に等しい `batch_size` が必要であり、残りの次元はアイテムのデータ仕様（再生バッファのコンストラクタに渡されたデータ仕様と同じ）に準拠していなければなりません。\n",
    "\n",
    "アイテムのバッチを追加する例を以下に示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "injured-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = tf.constant(1 * np.ones(\n",
    "    data_spec[0].shape.as_list(), dtype=np.float32))\n",
    "lidar = tf.constant(\n",
    "    2 * np.ones(data_spec[1][0].shape.as_list(), dtype=np.float32))\n",
    "camera = tf.constant(\n",
    "    3 * np.ones(data_spec[1][1].shape.as_list(), dtype=np.float32))\n",
    "\n",
    "values = (action, (lidar, camera))\n",
    "values_batched = tf.nest.map_structure(lambda t: tf.stack([t] * batch_size),\n",
    "                                       values)\n",
    "\n",
    "replay_buffer.add_batch(values_batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-denver",
   "metadata": {},
   "source": [
    "### バッファからの読み込み\n",
    "\n",
    "`TFUniformReplayBuffer` からデータを読み込む方法は 3 つあります。\n",
    "\n",
    "1. `get_next()` - バッファからサンプルを 1 つ返します。返されるサンプルバッチサイズとタイムステップ数は、このメソッドの引数で指定できます。\n",
    "2. `as_dataset()` - 再生バッファを tf.data.Dataset として返します。その後、データセットイテレータを作成し、バッファ内のアイテムのサンプルをイテレートできます。\n",
    "3. `gather_all()` - バッファ内のすべてのアイテムを形状 `[batch, time, data_spec]` を含むテンソルとして返します。\n",
    "\n",
    "上記の各メソッドを使用して再生バッファから読み込む例を以下に示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "royal-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more items to the buffer before reading\n",
    "for _ in range(5):\n",
    "    replay_buffer.add_batch(values_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "paperback-thought",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-836642ae3c7f>:3: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "# Get one sample from the replay buffer with batch size 10 and 1 timestep:\n",
    "\n",
    "sample = replay_buffer.get_next(sample_batch_size=10, num_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "swiss-collect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterator trajectories:\n",
      "[(TensorShape([4, 2, 3]), (TensorShape([4, 2, 5]), TensorShape([4, 2, 3, 2]))), (TensorShape([4, 2, 3]), (TensorShape([4, 2, 5]), TensorShape([4, 2, 3, 2]))), (TensorShape([4, 2, 3]), (TensorShape([4, 2, 5]), TensorShape([4, 2, 3, 2])))]\n"
     ]
    }
   ],
   "source": [
    "# Convert the replay buffer to a tf.data.Dataset and iterate through it\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=4,\n",
    "    num_steps=2)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "print(\"Iterator trajectories:\")\n",
    "trajectories = []\n",
    "for _ in range(3):\n",
    "    t, _ = next(iterator)\n",
    "    trajectories.append(t)\n",
    "\n",
    "print(tf.nest.map_structure(lambda t: t.shape, trajectories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cosmetic-sculpture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-763429bbe20f>:2: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n",
      "Trajectories from gather all:\n",
      "(TensorShape([32, 6, 3]), (TensorShape([32, 6, 5]), TensorShape([32, 6, 3, 2])))\n"
     ]
    }
   ],
   "source": [
    "# Read all elements in the replay buffer:\n",
    "trajectories = replay_buffer.gather_all()\n",
    "\n",
    "print(\"Trajectories from gather all:\")\n",
    "print(tf.nest.map_structure(lambda t: t.shape, trajectories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-worker",
   "metadata": {},
   "source": [
    "## `PyUniformReplayBuffer`\n",
    "\n",
    "`PyUniformReplayBuffer` は `TFUniformReplayBuffer` と同じ機能を持ちますが、データは `tf` 変数ではなく `numpy` 配列に格納されます。このバッファはグラフ外のデータ収集に使用されます。`numpy` にバッキングストレージを用意すると、`tf` 変数を使用せずに一部のアプリケーションがデータ操作（優先度を更新するためのインデックス作成など）を実行しやすくなります。ただし、この実装では Tensorflow を使用したグラフ最適化のメリットを享受できません。\n",
    "\n",
    "エージェントのポリシートラジェクトリの仕様から `PyUniformReplayBuffer` をインスタンス化する例を以下に示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "according-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer_capacity = 1000*32 # same capacity as the TFUniformReplayBuffer\n",
    "\n",
    "py_replay_buffer = py_uniform_replay_buffer.PyUniformReplayBuffer(\n",
    "    capacity=replay_buffer_capacity,\n",
    "    data_spec=tensor_spec.to_nest_array_spec(data_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-alias",
   "metadata": {},
   "source": [
    "## トレーニング中に再生バッファを使用する\n",
    "\n",
    "再生バッファを作成してアイテムを読み書きする方法がわかったので、エージェントのトレーニング中に再生バッファを使用してトラジェクトリを格納できるようになりました。\n",
    "\n",
    "### データ収集\n",
    "\n",
    "まず、データ収集中に再生バッファを使用する方法を見てみましょう。\n",
    "\n",
    "TF-Agents では、`Driver`（詳細は `Driver` のチュートリアルをご覧ください）を使用して環境内の経験を収集します。`Driver` を使用するには、`Driver` がトラジェクトリを受け取ったときに実行する関数である `Observer` を指定します。\n",
    "\n",
    "そのため、トラジェクトリ要素を再生バッファに追加するために `add_batch(items)` を呼び出すオブザーバーを追加してアイテム（のバッチ）を再生バッファに追加します。\n",
    "\n",
    "`TFUniformReplayBuffer` を使用した例を以下に示します。まず、環境、ネットワーク、エージェントを作成します。その後、`TFUniformReplayBuffer` を作成します。再生バッファ内のトラジェクトリ要素の仕様は、エージェントの収集データ仕様に等しくなっていることに注意してください。その後、その `add_batch` メソッドをトレーニング中にデータ収集を実行するドライバーのオブザーバーに設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "orange-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment\n",
    "env = suite_gym.load('CartPole-v0')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "circular-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network\n",
    "q_net = q_network.QNetwork(\n",
    "    tf_env.time_step_spec().observation,\n",
    "    tf_env.action_spec(),\n",
    "    fc_layer_params=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "honest-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=tf.compat.v1.train.AdamOptimizer(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "primary-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create replay buffer\n",
    "replay_buffer_capacity = 1000\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "clinical-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an observer that adds to the replay buffer\n",
    "replay_observer = [replay_buffer.add_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "applied-calgary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/nakayamayasuaki/python_env/dl_env/lib/python3.8/site-packages/tf_agents/drivers/dynamic_step_driver.py:196: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.while_loop(c, b, vars, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n"
     ]
    }
   ],
   "source": [
    "# Collect data\n",
    "collect_steps_per_iteration = 10\n",
    "collect_op = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.collect_policy,\n",
    "    observers=replay_observer,\n",
    "    num_steps=collect_steps_per_iteration).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-server",
   "metadata": {},
   "source": [
    "### トレーニングステップ用のデータ読み込み\n",
    "\n",
    "トラジェクトリ要素を再生バッファに追加した後は、再生バッファからトラジェクトリのバッチを読み取り、トレーニングステップの入力データとして使用できます。\n",
    "\n",
    "トレーニングループ内で再生バッファのトラジェクトリをトレーニングする方法の例を以下に示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "emerging-chemistry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/nakayamayasuaki/python_env/dl_env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    }
   ],
   "source": [
    "# Read the replay buffer as a Dataset,\n",
    "# read batches of 4 elements, each with 2 timesteps:\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=4,\n",
    "    num_steps=2)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "num_train_steps = 10\n",
    "\n",
    "for _ in range(num_train_steps):\n",
    "    trajectories, _ = next(iterator)\n",
    "    loss = agent.train(experience=trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-adult",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
