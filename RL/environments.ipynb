{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "boolean-activation",
   "metadata": {},
   "source": [
    "# [環境](https://www.tensorflow.org/agents/tutorials/2_environments_tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "southeast-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-agenda",
   "metadata": {},
   "source": [
    "## Python環境\n",
    "\n",
    "`step(action)`は与えられた環境で行動を適用し次のステップに関する以下の情報を返す。\n",
    "\n",
    "1. `observation`：これは、エージェントが次のステップで行動を選択するために観察できる環境状態の一部です。\n",
    "2. `reward`：エージェントは、複数のステップにわたってこれらの報酬の合計を最大化することを学習します。\n",
    "3. `step_type`：環境との相互作用は通常、シーケンス/エピソードの一部です (チェスのゲームで複数の動きがあるように)。step_type は、`FIRST`、`MID`または`LAST`のいずれかで、このタイムステップがシーケンスの最初、中間、または最後のステップかどうかを示します。\n",
    "4. `discount`：これは、現在のタイムステップでの報酬に対する次のタイムステップでの報酬の重み付けを表す浮動小数です。\n",
    "\n",
    "これらは、名前付きタプル`TimeStep(step_type, reward, discount, observation)`にグループ化されます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "statistical-liverpool",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1)\n",
      "time_step_spec.observation: BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n",
      "time_step_spec.step_type: ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')\n",
      "time_step_spec.discount: BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)\n",
      "time_step_spec.reward: ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
     ]
    }
   ],
   "source": [
    "environment = suite_gym.load('CartPole-v0')\n",
    "print('action_spec:', environment.action_spec())\n",
    "print('time_step_spec.observation:', environment.time_step_spec().observation)\n",
    "print('time_step_spec.step_type:', environment.time_step_spec().step_type)\n",
    "print('time_step_spec.discount:', environment.time_step_spec().discount)\n",
    "print('time_step_spec.reward:', environment.time_step_spec().reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afraid-lightning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.03754664, -0.01755501,  0.03016233,  0.03129518], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.03719554,  0.1771217 ,  0.03078823, -0.2517207 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.04073798,  0.3717908 ,  0.02575382, -0.53453565], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.04817379,  0.5665413 ,  0.01506311, -0.8189937 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.05950462,  0.76145387, -0.00131677, -1.106901  ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.0747337 ,  0.9565931 , -0.02345479, -1.3999968 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.09386556,  1.1519986 , -0.05145472, -1.6999195 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.11690553,  1.3476743 , -0.08545312, -2.0081656 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.14385901,  1.5435756 , -0.12561642, -2.326038  ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.17473052,  1.7395937 , -0.17213719, -2.6545825 ], dtype=float32))\n",
      "TimeStep(step_type=array(2, dtype=int32), reward=array(1., dtype=float32), discount=array(0., dtype=float32), observation=array([ 0.2095224 ,  1.9355377 , -0.22522883, -2.9945135 ], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "action = np.array(1, dtype=np.int32)\n",
    "time_step = environment.reset()\n",
    "print(time_step)\n",
    "while not time_step.is_last():\n",
    "    time_step = environment.step(action)\n",
    "    print(time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-settlement",
   "metadata": {},
   "source": [
    "### カスタム環境の作成\n",
    "\n",
    "例：簡単なブラックジャック\n",
    "\n",
    "1. ゲームは、1～10 の数値が付けられた無限のカード一式を使用してプレイします。\n",
    "2. 毎回、エージェントは2つの行動 (新しいランダムカードを取得する、またはその時点のラウンドを停止する) を実行できます。\n",
    "3. 目標はラウンド終了時にカードの合計を 21 にできるだけ近づけることです。\n",
    "\n",
    "この時の環境：\n",
    "\n",
    "1. `action`：2つの行動があります（行動0：新しいカードを取得、行動1：その時点のラウンドを終了）。\n",
    "2. `observation`：その時点のラウンドのカードの合計。\n",
    "3. `reward`：目標は、21 にできるだけ近づけることなので、ラウンド終了時に次の報酬を使用します。`sum_of_cards - 21 if sum_of_cards <= 21 else -21`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "desperate-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardGameEnv(py_environment.PyEnvironment):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
    "        self._state = 0\n",
    "        self._episode_ended = False\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = 0\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "\n",
    "    def _step(self, action):\n",
    "\n",
    "        if self._episode_ended:\n",
    "            # The last action ended the episode. Ignore the current action and start a new episode.\n",
    "            return self.reset()\n",
    "\n",
    "        # Make sure episodes don't go on forever.\n",
    "        if action == 1:\n",
    "            self._episode_ended = True\n",
    "        elif action == 0:\n",
    "            new_card = np.random.randint(1, 11)\n",
    "            self._state += new_card\n",
    "        else:\n",
    "            raise ValueError('`action` should be 0 or 1.')\n",
    "\n",
    "        if self._episode_ended or self._state >= 21:\n",
    "            reward = self._state - 21 if self._state <= 21 else -21\n",
    "            return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "        else:\n",
    "            return ts.transition(\n",
    "                np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-amber",
   "metadata": {},
   "source": [
    "環境の検証。生成された`observation`と`time_steps`が仕様で定義されている正しい形状とタイプに従っていることを確認する。\n",
    "\n",
    "以下ではランダムなポリシーをを5つのエピソードで実行している。問題があればエラーが返ってくる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "czech-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = CardGameEnv()\n",
    "utils.validate_py_environment(environment, episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-penetration",
   "metadata": {},
   "source": [
    "固定ポリシーを適用する。ここでは3枚のカードを要求して終了する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "expired-future",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([0], dtype=int32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([6], dtype=int32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([12], dtype=int32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([16], dtype=int32))\n",
      "TimeStep(step_type=array(2, dtype=int32), reward=array(-5., dtype=float32), discount=array(0., dtype=float32), observation=array([16], dtype=int32))\n",
      "Final Reward =  -5.0\n"
     ]
    }
   ],
   "source": [
    "get_new_card_action = np.array(0, dtype=np.int32)\n",
    "end_round_action = np.array(1, dtype=np.int32)\n",
    "\n",
    "environment = CardGameEnv()\n",
    "time_step = environment.reset()\n",
    "print(time_step)\n",
    "cumulative_reward = time_step.reward\n",
    "\n",
    "for _ in range(3):\n",
    "    time_step = environment.step(get_new_card_action)\n",
    "    print(time_step)\n",
    "    cumulative_reward += time_step.reward\n",
    "\n",
    "time_step = environment.step(end_round_action)\n",
    "print(time_step)\n",
    "cumulative_reward += time_step.reward\n",
    "print('Final Reward = ', cumulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-policy",
   "metadata": {},
   "source": [
    "一般的なラッパーは`environments/wrappers.py`にある。これらはPython環境`py_environment.PyEnvironment`を受け取り、変更された環境`py_environment.PyEnvironment`を返す。例：\n",
    "\n",
    "1. `ActionDiscretizeWrapper`：連続空間で定義された行動を離散化された行動に変換します。\n",
    "2. `RunStats`：実行したステップ数、完了したエピソード数など、環境の実行統計をキャプチャします。\n",
    "3. `TimeLimit`：一定のステップ数の後にエピソードを終了します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-combining",
   "metadata": {},
   "source": [
    "## TensorFlow環境\n",
    "\n",
    "`environments/tf_environment.TFEnvironment`で定義されている。以下の点でPython環境とは異なる。\n",
    "\n",
    "- 配列の代わりにテンソルオブジェクトを生成する\n",
    "- 仕様と比較したときに生成されたテンソルにバッチディメンションを追加します\n",
    "\n",
    "Python環境をTF環境に変換すると、TensorFlowで操作を並列化できます。たとえば、環境からデータを収集して`replay_buffer`に追加する`collect_experience_op`、および、`replay_buffer`から読み取り、エージェントをトレーニングする`train_op`を定義し、TensorFlowで自然に並列実行することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-protest",
   "metadata": {},
   "source": [
    "### カスタム環境の作成\n",
    "\n",
    "直接作るのは難しいので、Python環境をTensorFlow環境に`TFPyEnvironment`ラッパーを使って変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "clear-inspector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "TimeStep Specs: TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32)))\n",
      "Action Specs: BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "print(isinstance(tf_env, tf_environment.TFEnvironment))\n",
    "print(\"TimeStep Specs:\", tf_env.time_step_spec())\n",
    "print(\"Action Specs:\", tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fourth-locking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TimeStep(step_type=array([0], dtype=int32), reward=array([0.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.02313676, -0.03736085, -0.023516  ,  0.02066958]],\n",
      "      dtype=float32)), array([0], dtype=int32), TimeStep(step_type=array([1], dtype=int32), reward=array([1.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.02238954, -0.2321378 , -0.02310261,  0.30584118]],\n",
      "      dtype=float32))]\n",
      "[TimeStep(step_type=array([1], dtype=int32), reward=array([1.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.02238954, -0.2321378 , -0.02310261,  0.30584118]],\n",
      "      dtype=float32)), array([1], dtype=int32), TimeStep(step_type=array([1], dtype=int32), reward=array([1.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.01774678, -0.03669438, -0.01698579,  0.0059627 ]],\n",
      "      dtype=float32))]\n",
      "[TimeStep(step_type=array([1], dtype=int32), reward=array([1.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.01774678, -0.03669438, -0.01698579,  0.0059627 ]],\n",
      "      dtype=float32)), array([0], dtype=int32), TimeStep(step_type=array([1], dtype=int32), reward=array([1.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.0170129 , -0.23156866, -0.01686653,  0.29323837]],\n",
      "      dtype=float32))]\n",
      "Total reward: [3.]\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "# reset() creates the initial time_step after resetting the environment.\n",
    "time_step = tf_env.reset()\n",
    "num_steps = 3\n",
    "transitions = []\n",
    "reward = 0\n",
    "for i in range(num_steps):\n",
    "    action = tf.constant([i % 2])\n",
    "    # applies the action and returns the new TimeStep.\n",
    "    next_time_step = tf_env.step(action)\n",
    "    transitions.append([time_step, action, next_time_step])\n",
    "    reward += next_time_step.reward\n",
    "    time_step = next_time_step\n",
    "\n",
    "np_transitions = tf.nest.map_structure(lambda x: x.numpy(), transitions)\n",
    "print('\\n'.join(map(str, np_transitions)))\n",
    "print('Total reward:', reward.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "refined-tunisia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_episodes: 5 num_steps: 136\n",
      "avg_length 27.2 avg_reward: 27.2\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    while not time_step.is_last():\n",
    "        action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "        time_step = tf_env.step(action)\n",
    "        episode_steps += 1\n",
    "        episode_reward += time_step.reward.numpy()\n",
    "    rewards.append(episode_reward)\n",
    "    steps.append(episode_steps)\n",
    "    time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)\n",
    "\n",
    "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
    "print('avg_length', avg_length, 'avg_reward:', avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-argument",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
