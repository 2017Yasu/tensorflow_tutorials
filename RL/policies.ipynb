{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "growing-expression",
   "metadata": {},
   "source": [
    "# [ポリシー](https://www.tensorflow.org/agents/tutorials/3_policies_tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "executed-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import network\n",
    "\n",
    "from tf_agents.policies import py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import actor_policy\n",
    "from tf_agents.policies import q_policy\n",
    "from tf_agents.policies import greedy_policy\n",
    "\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-match",
   "metadata": {},
   "source": [
    "## Pythonポリシー\n",
    "\n",
    "このインターフェースは`policies/py_policy.PyPolicy`に定義されている。\n",
    "\n",
    "最も重要なメソッドは、環境からの観測を含む`time_step`を、次の属性を含む`PlicyStep`にマップする`action(time_step)`です。\n",
    "\n",
    "- `action` ：環境に適用されるアクション。\n",
    "- `state` ：次のアクション呼び出しに供給されるポリシーの状態（RNN状態など）。\n",
    "- `info` ：アクションログの確率などのオプションのサイド情報。\n",
    "\n",
    "`time_step_spec`と`action_spec`は、入力タイムステップと出力アクションの仕様を返す。\n",
    "\n",
    "ポリシーには、ステートフルポリシーの状態をリセットするために通常使用される`reset`機能もあります。\n",
    "\n",
    "`update(new_policy)`関数は、 `self`を`new_policy`に向けて更新します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-destruction",
   "metadata": {},
   "source": [
    "### 例1：ランダムなPythonポリシー\n",
    "\n",
    "入力された`time_step`は無視されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "promising-fisher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicyStep(action=array([ 1, -5], dtype=int32), state=(), info=())\n",
      "PolicyStep(action=array([4, 1], dtype=int32), state=(), info=())\n"
     ]
    }
   ],
   "source": [
    "action_spec = array_spec.BoundedArraySpec((2,), np.int32, -10, 10)\n",
    "my_random_py_policy = random_py_policy.RandomPyPolicy(time_step_spec=None,\n",
    "    action_spec=action_spec)\n",
    "time_step = None\n",
    "action_step = my_random_py_policy.action(time_step)\n",
    "print(action_step)\n",
    "action_step = my_random_py_policy.action(time_step)\n",
    "print(action_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-yeast",
   "metadata": {},
   "source": [
    "### 例2：スクリプト化されたPythonポリシー\n",
    "\n",
    "スクリプト化されたポリシーは、 `(num_repeats, action)`タプルのリストとして表されるアクションのスクリプトを再生します。 `action`関数が呼び出されるたびに、指定された回数の繰り返しが完了するまでリストから次のアクションが返され、リスト内の次のアクションに進みます。 `reset`メソッドを呼び出して、リストの最初から実行を開始できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adolescent-neighborhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing scripted policy...\n",
      "PolicyStep(action=array([5, 2], dtype=int32), state=[0, 1], info=())\n",
      "PolicyStep(action=array([1, 2], dtype=int32), state=[2, 1], info=())\n",
      "PolicyStep(action=array([1, 2], dtype=int32), state=[2, 2], info=())\n",
      "Resetting my_scripted_py_policy...\n",
      "PolicyStep(action=array([5, 2], dtype=int32), state=[0, 1], info=())\n"
     ]
    }
   ],
   "source": [
    "action_spec = array_spec.BoundedArraySpec((2,), np.int32, -10, 10)\n",
    "action_script = [(1, np.array([5, 2], dtype=np.int32)), \n",
    "                 (0, np.array([0, 0], dtype=np.int32)), # Setting `num_repeats` to 0 will skip this action.\n",
    "                 (2, np.array([1, 2], dtype=np.int32)), \n",
    "                 (1, np.array([3, 4], dtype=np.int32))]\n",
    "\n",
    "my_scripted_py_policy = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=None, action_spec=action_spec, action_script=action_script)\n",
    "\n",
    "policy_state = my_scripted_py_policy.get_initial_state()\n",
    "time_step = None\n",
    "print('Executing scripted policy...')\n",
    "action_step = my_scripted_py_policy.action(time_step, policy_state)\n",
    "print(action_step)\n",
    "action_step= my_scripted_py_policy.action(time_step, action_step.state)\n",
    "print(action_step)\n",
    "action_step = my_scripted_py_policy.action(time_step, action_step.state)\n",
    "print(action_step)\n",
    "\n",
    "print('Resetting my_scripted_py_policy...')\n",
    "policy_state = my_scripted_py_policy.get_initial_state()\n",
    "action_step = my_scripted_py_policy.action(time_step, policy_state)\n",
    "print(action_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-champagne",
   "metadata": {},
   "source": [
    "## TensorFlowポリシー\n",
    "\n",
    "TensorFlowポリシーは、Pythonポリシーと同じインターフェースに従います。いくつかの例を見てみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-portable",
   "metadata": {},
   "source": [
    "### 例1：ランダムTFポリシー\n",
    "\n",
    "RandomTFPolicyを使用して、特定の離散/連続`action_spec`に従ってランダムアクションを生成できます。入力された`time_step`は無視されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stone-decrease",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:\n",
      "tf.Tensor([-0.7680621  -0.31236172], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    (2,), tf.float32, minimum=-1, maximum=3)\n",
    "input_tensor_spec = tensor_spec.TensorSpec((2,), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "\n",
    "my_random_tf_policy = random_tf_policy.RandomTFPolicy(\n",
    "    action_spec=action_spec, time_step_spec=time_step_spec)\n",
    "observation = tf.ones(time_step_spec.observation.shape)\n",
    "time_step = ts.restart(observation)\n",
    "action_step = my_random_tf_policy.action(time_step)\n",
    "\n",
    "print('Action:')\n",
    "print(action_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-conflict",
   "metadata": {},
   "source": [
    "### 例2：Actor policy\n",
    "\n",
    "アクターポリシーは以下のいずれかのネットワークを使って作成することができる。\n",
    "\n",
    "- `time_steps`をアクションにマッピングしするネットワーク\n",
    "- `time_steps`をアクション上の確率分布にマッピングするネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-theater",
   "metadata": {},
   "source": [
    "#### Action networkを使用する\n",
    "\n",
    "この例では、アクションテンソルを生成するアクションネットワークを使用してポリシーを作成する。この場合、`policy.distribution(time_step)`は`policy.action(time_step)`の出力に関する決定論的（デルタ）分布を返す。確率的ポリシーを生成したい場合は、アクションにノイズを追加するポリシーラッパーでアクターポリシーをラップする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "spare-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNet(network.Network):\n",
    "\n",
    "    def __init__(self, input_tensor_spec, output_tensor_spec):\n",
    "        super(ActionNet, self).__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),\n",
    "            name='ActionNet')\n",
    "        self._output_tensor_spec = output_tensor_spec\n",
    "        self._sub_layers = [\n",
    "            tf.keras.layers.Dense(action_spec.shape.num_elements(), activation=tf.nn.tanh),\n",
    "        ]\n",
    "\n",
    "    def call(self, observations, step_type, network_state):\n",
    "        del step_type\n",
    "\n",
    "        output = tf.cast(observations, dtype=tf.float32)\n",
    "        for layer in self._sub_layers:\n",
    "            output = layer(output)\n",
    "        actions = tf.reshape(output, [-1] + self._output_tensor_spec.shape.as_list())\n",
    "\n",
    "        # Scale and shift actions to the correct range if necessary.\n",
    "        return actions, network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "isolated-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_spec = tensor_spec.TensorSpec((4,), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "action_spec = tensor_spec.BoundedTensorSpec((3,), tf.float32, minimum=-1, maximum=1)\n",
    "\n",
    "action_net = ActionNet(input_tensor_spec, action_spec)\n",
    "\n",
    "my_actor_policy = actor_policy.ActorPolicy(\n",
    "    time_step_spec=time_step_spec,\n",
    "    action_spec=action_spec,\n",
    "    actor_network=action_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hairy-heavy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations:\n",
      " tf.Tensor(\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]], shape=(2, 4), dtype=float32)\n",
      "Initial time step:\n",
      " TimeStep(step_type=<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>, reward=<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>, discount=<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>, observation=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1.]], dtype=float32)>)\n",
      "Action:\n",
      "tf.Tensor(\n",
      "[[-0.41495764 -0.7019905   0.13005573]\n",
      " [-0.41495764 -0.7019905   0.13005573]], shape=(2, 3), dtype=float32)\n",
      "Action distribution:\n",
      "tfp.distributions.Deterministic(\"Deterministic\", batch_shape=[2, 3], event_shape=[], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# We can apply an arbitrary batch of `times_steps` to this actor policy\n",
    "# (`time_steps` must follow the `time_step_spec`)\n",
    "\n",
    "batch_size = 2\n",
    "observations = tf.ones([2] + time_step_spec.observation.shape.as_list())\n",
    "print('Observations:\\n', observations)\n",
    "\n",
    "time_step = ts.restart(observations, batch_size)\n",
    "print('Initial time step:\\n', time_step)\n",
    "\n",
    "action_step = my_actor_policy.action(time_step)\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "\n",
    "distribution_step = my_actor_policy.distribution(time_step)\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-adobe",
   "metadata": {},
   "source": [
    "#### Action distribution networkを使用する\n",
    "\n",
    "これを使っても確率的ポリシーを生成することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "annoying-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionDistributionNet(ActionNet):\n",
    "\n",
    "    def call(self, observations, step_type, network_state):\n",
    "        action_means, network_state = super(ActionDistributionNet, self).call(\n",
    "            observations, step_type, network_state)\n",
    "\n",
    "        action_std = tf.ones_like(action_means)\n",
    "        return tfp.distributions.MultivariateNormalDiag(action_means, action_std), network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "super-variation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:\n",
      "tf.Tensor(\n",
      "[[-1.         -0.266338   -0.15280703]\n",
      " [-0.26316485 -1.         -0.532077  ]], shape=(2, 3), dtype=float32)\n",
      "Action distribution:\n",
      "tfp.distributions.MultivariateNormalDiag(\"ActionNet_MultivariateNormalDiag\", batch_shape=[2], event_shape=[3], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "action_distribution_net = ActionDistributionNet(input_tensor_spec, action_spec)\n",
    "\n",
    "my_actor_policy = actor_policy.ActorPolicy(\n",
    "    time_step_spec=time_step_spec,\n",
    "    action_spec=action_spec,\n",
    "    actor_network=action_distribution_net)\n",
    "\n",
    "action_step = my_actor_policy.action(time_step)\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "distribution_step = my_actor_policy.distribution(time_step)\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-swift",
   "metadata": {},
   "source": [
    "アクションの値を与えた`action_spec`内であることを保証する場合は、`ActorPolicy`のコンストラクタで引数に`clip=True`を指定する（デフォルト）。そうしたくなければ、`clip=False`を指定する。\n",
    "\n",
    "確率的ポリシーを決定論的ポリシーに変換したい場合は、`GreedyPolicy`ラッパーを使って実現できる。これは`stochastic_policy.distribution().mode()`をアクションとし、それらの決定論的（デルタ）分布を`distribution()`として選択する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-woman",
   "metadata": {},
   "source": [
    "### 例3：Qポリシー\n",
    "\n",
    "Qポリシーは、DQNなどのエージェントで使用され、個別のアクションごとにQ値を予測するQネットワークに基づいています。特定のタイムステップで、Qポリシーのアクション分布は、q値をロジットとして使用して作成されたカテゴリ分布です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "peripheral-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_spec = tensor_spec.TensorSpec((4,), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "action_spec = tensor_spec.BoundedTensorSpec((),\n",
    "                                            tf.int32,\n",
    "                                            minimum=0,\n",
    "                                            maximum=2)\n",
    "num_actions = action_spec.maximum - action_spec.minimum + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adequate-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(network.Network):\n",
    "\n",
    "    def __init__(self, input_tensor_spec, action_spec, num_actions=num_actions, name=None):\n",
    "        super(QNetwork, self).__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),\n",
    "            name=name)\n",
    "        self._sub_layers = [\n",
    "            tf.keras.layers.Dense(num_actions),\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs, step_type=None, network_state=()):\n",
    "        del step_type\n",
    "        inputs = tf.cast(inputs, tf.float32)\n",
    "        for layer in self._sub_layers:\n",
    "            inputs = layer(inputs)\n",
    "        return inputs, network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "reported-crossing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:\n",
      "tf.Tensor([2 0], shape=(2,), dtype=int32)\n",
      "Action distribution:\n",
      "tfp.distributions.Categorical(\"Categorical\", batch_shape=[2], event_shape=[], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "observation = tf.ones([batch_size] + time_step_spec.observation.shape.as_list())\n",
    "time_steps = ts.restart(observation, batch_size=batch_size)\n",
    "\n",
    "my_q_network = QNetwork(\n",
    "    input_tensor_spec=input_tensor_spec,\n",
    "    action_spec=action_spec)\n",
    "my_q_policy = q_policy.QPolicy(\n",
    "    time_step_spec, action_spec, q_network=my_q_network)\n",
    "action_step = my_q_policy.action(time_steps)\n",
    "distribution_step = my_q_policy.distribution(time_steps)\n",
    "\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-bowling",
   "metadata": {},
   "source": [
    "## ポリシーラッパー\n",
    "\n",
    "ポリシーラッパーを使用して、特定のポリシーを変更できます（ノイズの追加など）。ポリシーラッパーはポリシー（Python / TensorFlow）のサブクラスであるため、他のポリシーと同じように使用できます。\n",
    "\n",
    "### 例：Greedy policy\n",
    "Greedyラッパーを使用して、 `distribution()`を実装しているTensorFlowポリシーをラップできます。\n",
    "- `GreedyPolicy.action()`は`wrapped_policy.distribution().mode()`を返し、\n",
    "- `GreedyPolicy.distribution()`は`GreedyPolicy.action()`上の決定論的/デルタ分布です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "southeast-religion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:\n",
      "tf.Tensor([0 0], shape=(2,), dtype=int32)\n",
      "Action distribution:\n",
      "tfp.distributions.DeterministicWithLogProb(\"Deterministic\", batch_shape=[2], event_shape=[], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "my_greedy_policy = greedy_policy.GreedyPolicy(my_q_policy)\n",
    "\n",
    "action_step = my_greedy_policy.action(time_steps)\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "\n",
    "distribution_step = my_greedy_policy.distribution(time_steps)\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-elevation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
